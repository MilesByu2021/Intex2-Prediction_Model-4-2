{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('ml-venv')",
   "metadata": {
    "interpreter": {
     "hash": "5bf28ef58c3ee5a41067683c53f69731b4fc1a48222c72ab44f0d32226bfe484"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "### Functions\n",
    "\n",
    "# ML PIPELINE\n",
    "\n",
    "# 1. DATA INGESTION (READING IN DATA)\n",
    "## Function to get data from a csv file\n",
    "def get_data(url, drop=[]):\n",
    "    import pandas as pd\n",
    "    \n",
    "    df = pd.read_csv(url)\n",
    "    if len(drop) > 0:\n",
    "        for col in drop:\n",
    "            df.drop(columns=[col], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# 2. DATA CLEANING\n",
    "## Function to group data in 'Other' category if less than a certain percentage of total\n",
    "def bin_groups(df, percent):\n",
    "    import pandas as pd\n",
    "\n",
    "    for col in df:\n",
    "        if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "            for group, count in df[col].value_counts().iteritems():\n",
    "                if count / len(df) < percent:\n",
    "                    df.loc[df[col] == group, col] = 'Other'\n",
    "    return df\n",
    "\n",
    "## drop columns that are missing a certain percentage of data\n",
    "def drop_columns_missing_data(df, cutoff):\n",
    "    for col in df:\n",
    "        if df[col].isna().sum() / len(df) > cutoff:\n",
    "            df.drop(columns=[col], inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "## Replace nulls with mean\n",
    "def impute_mean(df):\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    # Dummy code first; categorical features not allowed\n",
    "    for col in df:\n",
    "        if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "            df = pd.get_dummies(df, columns=[col], drop_first=True)\n",
    "\n",
    "    # Change the strategy to mean, median, or mode\n",
    "    imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    df = pd.DataFrame(imp.fit_transform(df), columns=df.columns)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "## Replace nulls with predicted values using KNN (KNN is similar to clustering, only use on numerical data)\n",
    "def impute_KNN(df):\n",
    "    from sklearn.impute import KNNImputer\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    import pandas as pd\n",
    "\n",
    "    # Dummy code first; categorical features not allowed\n",
    "    for col in df:\n",
    "        if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "            df = pd.get_dummies(df, columns=[col], drop_first=True)\n",
    "\n",
    "    # Clustering is biased by unstandardized data; so MinMax scale it\n",
    "    df = pd.DataFrame(MinMaxScaler().fit_transform(df), columns = df.columns)\n",
    "\n",
    "    imp = KNNImputer(n_neighbors=5, weights=\"uniform\")\n",
    "    df = pd.DataFrame(imp.fit_transform(df), columns=df.columns)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Replace nulls with predicted values using MLR\n",
    "def impute_reg(df):\n",
    "    from sklearn.experimental import enable_iterative_imputer\n",
    "    from sklearn.impute import IterativeImputer\n",
    "    import pandas as pd\n",
    "\n",
    "# Dummy code first; categorical features not allowed\n",
    "    for col in df:\n",
    "        if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "            df = pd.get_dummies(df, columns=[col], drop_first=True)\n",
    "\n",
    "    # Scaling is unnecessary for regression-based imputation\n",
    "\n",
    "    imp = IterativeImputer(max_iter=10, random_state=12345)\n",
    "    df = pd.DataFrame(imp.fit_transform(df), columns=df.columns)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# 3. Model Data\n",
    "\n",
    "def fit_mlr(df, test_size, random_state, label):\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import pandas as pd\n",
    "\n",
    "    X = df.drop(label,axis=1)\n",
    "    y = df[label]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    model = LinearRegression().fit(X_train, y_train)\n",
    "    print(f'R-squared (mlr): \\t{model.score(X_test, y_test)}')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "## Cross validation is a way to evaluate the performance of a model. Similar to training and testing your data. Just a little better way to do it. Newer too\n",
    "def fit_crossvalidate_mlr(df, k, label, repeat=True):\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.model_selection import KFold, RepeatedKFold, cross_val_score\n",
    "    import pandas as pd\n",
    "    from numpy import mean, std\n",
    "    X = df.drop(label,axis=1)\n",
    "    y = df[label]\n",
    "\n",
    "    # prepare the cross-validation procedure\n",
    "    if repeat:\n",
    "        # splits will be how to split the data and repeats is how many times it will run so a split of 5 and a repeat of 4 would have 20 total models\n",
    "        cv = RepeatedKFold(n_splits=k, n_repeats=5, random_state=12345)\n",
    "    else:\n",
    "        cv = KFold(n_splits=k, random_state=12345, shuffle=True)\n",
    "\n",
    "\n",
    "    # evaluate model\n",
    "    scores = cross_val_score(LinearRegression(), X, y, scoring='r2', cv=cv, n_jobs=-1)\n",
    "\n",
    "    # report performance\n",
    "    print(f'R-squared scores: \\n{scores}\\n')\n",
    "    print(f'Average R-squared:\\t{mean(scores)}')\n",
    "\n",
    "    return LinearRegression().fit(X, y)\n",
    "\n",
    "\n",
    "# 4. Deploy Model\n",
    "\n",
    "## Save model\n",
    "### pickle and joblib are two different packages you can use to save and load your models\n",
    "def dump_pickle(model, file_name):\n",
    "    import pickle\n",
    "    pickle.dump(model, open(file_name, \"wb\"))\n",
    "\n",
    "def dump_joblib(model, file_name):\n",
    "    import joblib\n",
    "    joblib.dump(model, file_name)\n",
    "\n",
    "# Load models\n",
    "def load_pickle(file_name):\n",
    "    import pickle\n",
    "    model = pickle.load(open(file_name, \"rb\"))\n",
    "    return model\n",
    "    \n",
    "def load_joblib(file_name):\n",
    "    import joblib\n",
    "    model = joblib.load(file_name)\n",
    "    return model\n",
    "\n",
    "# Downcasting for smaller dataset\n",
    "# Let's load the first dataset. We can us C engine to load faster those big files and take pandas function memory_usage to see what we gain. A conversion from bytes to megabytes is needed. So a division by 1024 ** 2 is need.\n",
    "def memory(df):\n",
    "    if isinstance(df,pd.DataFrame):\n",
    "        value = df.memory_usage(deep=True).sum() / 1024 ** 2\n",
    "    else: # we assume if not a df it's a series\n",
    "        value = df.memory_usage(deep=True) / 1024 ** 2\n",
    "    return value, \"{:03.2f} MB\".format(value)\n",
    "print(\"Done\")\n",
    "\n",
    "def fs_variance(df, label=\"\", p=0.02):\n",
    "  from sklearn.feature_selection import VarianceThreshold\n",
    "  import pandas as pd\n",
    "\n",
    "  if label != \"\":\n",
    "    X = df.drop(columns=[label])\n",
    "    \n",
    "  sel = VarianceThreshold(threshold=(p * (1 - p)))\n",
    "  sel.fit_transform(X)\n",
    "\n",
    "  # Add the label back in after removing poor features\n",
    "  return df[sel.get_feature_names_out()].join(df[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "13.48 MB\n13.48 MB\nGain:  100.0\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 252498 entries, 0 to 252497\nData columns (total 23 columns):\n #   Column                         Non-Null Count   Dtype  \n---  ------                         --------------   -----  \n 0   MILEPOINT                      252479 non-null  float64\n 1   LAT_UTM_Y                      252487 non-null  float64\n 2   LONG_UTM_X                     252487 non-null  float64\n 3   CRASH_SEVERITY_ID              252498 non-null  uint8  \n 4   WORK_ZONE_RELATED              252498 non-null  bool   \n 5   PEDESTRIAN_INVOLVED            252498 non-null  bool   \n 6   BICYCLIST_INVOLVED             252498 non-null  bool   \n 7   MOTORCYCLE_INVOLVED            252498 non-null  bool   \n 8   IMPROPER_RESTRAINT             252498 non-null  bool   \n 9   UNRESTRAINED                   252498 non-null  bool   \n 10  DUI                            252498 non-null  bool   \n 11  INTERSECTION_RELATED           252498 non-null  bool   \n 12  WILD_ANIMAL_RELATED            252498 non-null  bool   \n 13  DOMESTIC_ANIMAL_RELATED        252498 non-null  bool   \n 14  OVERTURN_ROLLOVER              252498 non-null  bool   \n 15  COMMERCIAL_MOTOR_VEH_INVOLVED  252498 non-null  bool   \n 16  TEENAGE_DRIVER_INVOLVED        252498 non-null  bool   \n 17  OLDER_DRIVER_INVOLVED          252498 non-null  bool   \n 18  NIGHT_DARK_CONDITION           252498 non-null  bool   \n 19  SINGLE_VEHICLE                 252498 non-null  bool   \n 20  DISTRACTED_DRIVING             252498 non-null  bool   \n 21  DROWSY_DRIVING                 252498 non-null  bool   \n 22  ROADWAY_DEPARTURE              252498 non-null  bool   \ndtypes: bool(19), float64(3), uint8(1)\nmemory usage: 10.6 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/Users/milesleung/Downloads/Utah_Crash_Data_2020.csv\")\n",
    "\n",
    "# data cleaning\n",
    "df['CRASH_ID'] = df['CRASH_ID'].astype('int')\n",
    "df['WORK_ZONE_RELATED'] = df['WORK_ZONE_RELATED'].astype('bool')\n",
    "df['CRASH_DATETIME'] = pd.to_datetime(df['CRASH_DATETIME'])\n",
    "\n",
    "dfIntSelection = df.select_dtypes(include=['int', 'float', 'bool', 'datetime64[ns]'])\n",
    "# Drop object\n",
    "df = dfIntSelection.apply(pd.to_numeric,downcast='unsigned')\n",
    "memInt, memIntTxt =  memory(df)\n",
    "memIntDownCast, memIntDownCastTxt = memory(df)\n",
    "\n",
    "print(memIntTxt)\n",
    "print(memIntDownCastTxt)\n",
    "print('Gain: ', memInt/memIntDownCast *100.0)\n",
    "\n",
    "df = df.drop(columns=['CRASH_ID', 'CRASH_DATETIME'])\n",
    "df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data loaded\n",
      "Data cleaned\n",
      "R-squared scores: \n",
      "[0.19305632 0.18511712 0.19544339 0.17882976 0.18623404 0.1909579\n",
      " 0.19291684 0.19548581 0.18233355 0.1772825  0.17624787 0.18670682\n",
      " 0.18935961 0.1946511  0.18555851 0.19510045 0.19082771 0.18516184\n",
      " 0.18737322 0.18733167 0.17931693 0.18559323 0.19747439 0.18906989\n",
      " 0.18148074 0.19447947 0.19121506 0.18953269 0.18481357 0.18516478\n",
      " 0.19797939 0.1878141  0.18129101 0.19293733 0.18903869 0.18885794\n",
      " 0.17942117 0.18882912 0.18237397 0.18977888 0.19359606 0.18025075\n",
      " 0.20101302 0.19941783 0.16694657 0.19080521 0.19120226 0.18920534\n",
      " 0.18091643 0.18391184]\n",
      "\n",
      "Average R-squared:\t0.1877940742861871\n",
      "model trained\n"
     ]
    }
   ],
   "source": [
    "# Pipeline Option 2: Mean-substituted missing values\n",
    "# Ingest data\n",
    "df_mean = df\n",
    "print('Data loaded')\n",
    "\n",
    "# Bin categorical group values that are too rare in the dataset\n",
    "# Drop any column with more than 50 percent of its data missing.\n",
    "df_mean = drop_columns_missing_data(df_mean, .5)\n",
    "# Replace the missing value with the mean, median, or mode of the column. Only works with numeric data.\n",
    "df_mean = impute_mean(df_mean)\n",
    "print('Data cleaned')\n",
    "\n",
    "model_mlr = fit_crossvalidate_mlr(df_mean, 10, \"CRASH_SEVERITY_ID\", True)\n",
    "print('model trained')\n",
    "\n",
    "dump_pickle(model_mlr, \"model_pickle_mean_prediction.sav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "4.88"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "# Input 22 features into X\n",
    "model = load_pickle(\"model_pickle_mean_prediction.sav\")\n",
    "round(model.predict([[12.702,4502005.00,414272.00,0,1,0,1,1,0,1,0,1,1,0,1,1,1,1,1,1,1,0]])[0], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data loaded\n",
      "Data cleaned\n",
      "(252498, 22)\n",
      "(252498, 16)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=16, step=1)"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import pandas as pd\n",
    "  \n",
    "# Import data and follow prior cleaning steps\n",
    "df_mean = df\n",
    "print('Data loaded')\n",
    "df_mean = bin_groups(df_mean, percent=0.05)\n",
    "df_mean = drop_columns_missing_data(df_mean, .5)\n",
    "df_mean = impute_mean(df_mean)\n",
    "print('Data cleaned')\n",
    "  \n",
    "# Select features only\n",
    "X = df_mean.drop(columns=['CRASH_SEVERITY_ID'])\n",
    "  \n",
    "# View the total number of features before reducing\n",
    "print(X.shape)\n",
    "  \n",
    "# Calculate variance based on desired p-value so that Variance = p(1 - p)\n",
    "# The code below uses p = 0.8 as the cutoff value:\n",
    "p = 0.02\n",
    "sel = VarianceThreshold(threshold=(p * (1 - p)))\n",
    "sel.fit_transform(X)\n",
    "X_reduced = pd.DataFrame(sel.fit_transform(X))\n",
    "  \n",
    "# View the remaining number of features\n",
    "print(X_reduced.shape)\n",
    "  \n",
    "# Notice that columns are dropped\n",
    "X_reduced.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['MILEPOINT', 'LAT_UTM_Y', 'LONG_UTM_X', 'WORK_ZONE_RELATED',\n",
       "       'UNRESTRAINED', 'DUI', 'INTERSECTION_RELATED', 'WILD_ANIMAL_RELATED',\n",
       "       'OVERTURN_ROLLOVER', 'COMMERCIAL_MOTOR_VEH_INVOLVED',\n",
       "       'TEENAGE_DRIVER_INVOLVED', 'OLDER_DRIVER_INVOLVED',\n",
       "       'NIGHT_DARK_CONDITION', 'SINGLE_VEHICLE', 'DISTRACTED_DRIVING',\n",
       "       'ROADWAY_DEPARTURE'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "# To see which columns were retained:\n",
    "sel.get_feature_names_out()\n",
    "df_mean[sel.get_feature_names_out()].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data loaded\n",
      "Data cleaned\n",
      "R-squared scores: \n",
      "[0.08728431 0.08874424 0.08920897 0.08198084 0.08475134 0.08601796\n",
      " 0.08821824 0.08992969 0.08406669 0.08374945 0.08006488 0.09254788\n",
      " 0.08020909 0.08719143 0.08167512 0.08712117 0.09110135 0.085459\n",
      " 0.08836454 0.09046554 0.08346216 0.07850712 0.09401876 0.08699498\n",
      " 0.08618142 0.09017861 0.08769236 0.0905389  0.0825718  0.08375358\n",
      " 0.09427021 0.08977227 0.08269507 0.08684079 0.08515024 0.08521971\n",
      " 0.08256666 0.08946789 0.08274005 0.08542111 0.0927442  0.08154326\n",
      " 0.09948789 0.08649743 0.07869464 0.08358456 0.0825944  0.08495549\n",
      " 0.08756879 0.08539465]\n",
      "\n",
      "Average R-squared:\t0.08638521473192762\n"
     ]
    }
   ],
   "source": [
    "# Pipeline choreography\n",
    "# Data cleaning and preparation pipeline\n",
    "df_mean = df\n",
    "print('Data loaded')\n",
    "df_mean = bin_groups(df_mean, percent=0.05)\n",
    "df_mean = drop_columns_missing_data(df_mean, .5)\n",
    "df_mean = impute_mean(df_mean)\n",
    "print('Data cleaned')\n",
    "\n",
    "# Feature selection and modeling pipeline\n",
    "df_mean = fs_variance(df_mean, label=\"CRASH_SEVERITY_ID\", p=.02)\n",
    "model = fit_crossvalidate_mlr(df_mean, 10, \"CRASH_SEVERITY_ID\", repeat=True)\n",
    "\n",
    "# Deployment pipeline\n",
    "dump_pickle(model, 'saved_model_prediction.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data loaded\n",
      "Data cleaned\n",
      "             R-squared\n",
      "HG Boost      0.141068\n",
      "XGBoost       0.130058\n",
      "Grad. Boost   0.120411\n",
      "Bayesian      0.086421\n",
      "Ridge         0.086421\n",
      "OLS           0.086421\n",
      "Lasso         0.000550\n",
      "Gamma        -0.000010\n",
      "Inverse      -0.000010\n",
      "Poisson      -0.000010\n",
      "LARS         -0.000010\n",
      "AdaBoost DT  -0.121998\n",
      "Deploy/stored the model done\n"
     ]
    }
   ],
   "source": [
    "### Finding the best Regression Model\n",
    "\n",
    "def fit_crossvalidate_reg(df, label, k=10, r=5, repeat=True):\n",
    "    import sklearn.linear_model as lm, pandas as pd, sklearn.ensemble as se\n",
    "    from sklearn.model_selection import KFold, RepeatedKFold, cross_val_score\n",
    "    from numpy import mean, std\n",
    "    from sklearn import svm\n",
    "    from sklearn import gaussian_process\n",
    "    from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\n",
    "    from xgboost import XGBRegressor\n",
    "    from sklearn import neighbors\n",
    "\n",
    "    ## Setting labels and features\n",
    "    X = df.drop(columns=[label])\n",
    "    y = df[label]\n",
    "\n",
    "    if repeat:\n",
    "        cv = RepeatedKFold(n_splits=k, n_repeats=r, random_state=12345)\n",
    "    else:\n",
    "        cv = KFold(n_splits=k, random_state=12345, shuffle=True)\n",
    "\n",
    "    fit = {}    # Use this to store each of the fit metrics\n",
    "    models = {} # Use this to store each of the models\n",
    "\n",
    "    # Create the model objects\n",
    "    model_ols = lm.LinearRegression()\n",
    "    model_rr = lm.Ridge(alpha=0.5) # adjust this alpha parameter for better results (between 0 and 1)\n",
    "    model_lr = lm.Lasso(alpha=0.1) # adjust this alpha parameter for better results (between 0 and 1)\n",
    "    model_llr = lm.LassoLars(alpha=0.1) # adjust this alpha parameter for better results (between 0 and 1)\n",
    "    model_br = lm.BayesianRidge()\n",
    "    model_pr = lm.TweedieRegressor(power=1, link=\"log\") # Power=1 means this is a Poisson\n",
    "    model_gr = lm.TweedieRegressor(power=2, link=\"log\") # Power=2 means this is a Gamma\n",
    "    model_igr = lm.TweedieRegressor(power=3) # Power=3 means this is an inverse Gamma\n",
    "    model_abr = se.AdaBoostRegressor(n_estimators=100, random_state=12345)\n",
    "    model_gbr = se.GradientBoostingRegressor(random_state=12345)\n",
    "    model_hgbr = se.HistGradientBoostingRegressor(random_state=12345)\n",
    "    model_xgb = XGBRegressor(n_estimators=1000, max_depth=7, eta=0.1, subsample=0.7, colsample_bytree=0.8)\n",
    "\n",
    "    # Fit a crss-validated R squared score and add it to the dict\n",
    "    fit['OLS'] = mean(cross_val_score(model_ols, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "    fit['Ridge'] = mean(cross_val_score(model_rr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "    fit['Lasso'] = mean(cross_val_score(model_lr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "    fit['LARS'] = mean(cross_val_score(model_llr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "    fit['Bayesian'] = mean(cross_val_score(model_br, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "    fit['Poisson'] = mean(cross_val_score(model_pr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "    fit['Gamma'] = mean(cross_val_score(model_gr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "    fit['Inverse'] = mean(cross_val_score(model_igr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "    fit['AdaBoost DT'] = mean(cross_val_score(model_abr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "    fit['Grad. Boost'] = mean(cross_val_score(model_gbr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "    fit['HG Boost'] = mean(cross_val_score(model_hgbr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "    fit['XGBoost'] = mean(cross_val_score(model_xgb, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "\n",
    "    # Add the model to another dict; make sure the keys have the same names as the list above\n",
    "    models['OLS'] = model_ols\n",
    "    models['Ridge'] = model_rr\n",
    "    models['Lasso'] = model_lr\n",
    "    models['LARS'] = model_llr\n",
    "    models['Bayesian'] = model_br\n",
    "    models['Poisson'] = model_pr\n",
    "    models['Gamma'] = model_gr\n",
    "    models['Inverse'] = model_igr\n",
    "    models['AdaBoost DT'] = model_abr\n",
    "    models['Grad. Boost'] = model_gbr\n",
    "    models['HG Boost'] = model_hgbr\n",
    "    models['XGBoost'] = model_xgb\n",
    "\n",
    "\n",
    "    # Add the fit dictionary to a new DataFrame, sort, extract the top row, use it to retrieve the model object from the models dictionary\n",
    "    df_fit = pd.DataFrame({'R-squared':fit})\n",
    "    df_fit.sort_values(by=['R-squared'], ascending=False, inplace=True)\n",
    "    best_model = df_fit.index[0]\n",
    "    print(df_fit)\n",
    "\n",
    "    return models[best_model].fit(X, y)\n",
    "\n",
    "# Data cleaning and preparation pipeline\n",
    "df_mean = df\n",
    "print('Data loaded')\n",
    "df_mean = bin_groups(df_mean, percent=0.05)\n",
    "df_mean = drop_columns_missing_data(df_mean, .5)\n",
    "df_mean = impute_mean(df_mean)\n",
    "print('Data cleaned')\n",
    "\n",
    "# Feature selection and modeling pipeline\n",
    "df_mean = fs_variance(df_mean, label=\"CRASH_SEVERITY_ID\", p=.02)\n",
    "model = fit_crossvalidate_reg(df_mean, 'CRASH_SEVERITY_ID', 5, 2)\n",
    "\n",
    "# Deploy/store the model\n",
    "dump_pickle(model, 'best_reg_model_prediction.sav')\n",
    "print('Deploy/stored the model done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   Actual SalePrice  Predicted Crash Severity ID\n0               2.0                     2.070355\n1               1.0                     1.372442\n2               1.0                     1.268721\n3               3.0                     2.550309\n4               1.0                     1.286885\n"
     ]
    }
   ],
   "source": [
    "model = load_pickle('best_reg_model_prediction.sav')\n",
    "print(pd.DataFrame({'Actual SalePrice':df_mean.CRASH_SEVERITY_ID, 'Predicted Crash Severity ID':model.predict(df_mean.drop(columns=['CRASH_SEVERITY_ID']))}).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert into ONNX format\n",
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "initial_type = [('float_input', FloatTensorType([None, 16]))]\n",
    "onx = convert_sklearn(model, initial_types=initial_type)\n",
    "with open(\"UtahCrash_HG_Boost_prediction.onnx\", \"wb\") as f:\n",
    "    f.write(onx.SerializeToString())"
   ]
  }
 ]
}